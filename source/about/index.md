---
title: about
date: 2021-12-06 22:26:39
---
This is Jinghong Chen, I am currently doing my MEng at the University of Cambridge. You can call me Eric ;)

I studied at HKU for a year before coming to Cambridge. They are different in almost all dimensions. At HKU, I was interested in robotics, especially embedded system and control. I was very invested in a robotic competition, which in hindsight somewhat burnt out my patience with hardware (probably because I realized I could never build an iron man suit myself). Now I am interested in Machine Learning, primarily in the field of NLP. The goal is to build *J.A.R.V.I.S*, if possible at all.

I am currently doing research with Prof. Bill Byrne on Natural Language Generation, an intriguing topic which has seen a number of successful models in recent years (GPT-2, T5, etc). However, it is both disconcerting and exciting that we don't know why these models work. Did the model learn to infer from "common sense"? What are the limiting factors of performance (number of parameters or model architecture)? And finally, how far are we from general intelligence (i.e., building *J.A.R.V.I.S*)? These are the questions that motivate me at the moment.

There is a feverish fascination and optimism about A.I today. But we need to remember Alexander Pope's beautiful and powerful remark:

*A little learning is a dangerous thing; Drink deep, or taste not the Pierian spring: There shallow draughts intoxicate the brain, And drinking largely sobers us again.*

Keep drinking.