

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="The personal blog of Jinghong Chen (Eric). On Machine Learning, Programming and beyond">
  <meta name="author" content="Jinghong Chen">
  <meta name="keywords" content="Machine Learning Jinghong Chen Eric Programming blog">
  <meta name="description" content="Excerpts from https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;quicktour Quick TourLet’s have a quick look at the huggingface 🤗 Transformers library features. The library downloads pretrained models for Natu">
<meta property="og:type" content="article">
<meta property="og:title" content="Huggingface transformers">
<meta property="og:url" content="http://erichen0615.github.io/2021/10/01/Huggingface-transformers/index.html">
<meta property="og:site_name" content="Eric Chen&#39;s Blog">
<meta property="og:description" content="Excerpts from https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;quicktour Quick TourLet’s have a quick look at the huggingface 🤗 Transformers library features. The library downloads pretrained models for Natu">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-10-01T00:00:00.000Z">
<meta property="article:modified_time" content="2021-12-09T08:21:25.980Z">
<meta property="article:author" content="Jinghong Chen">
<meta property="article:tag" content="Machine Learning Jinghong Chen Eric Programming blog">
<meta name="twitter:card" content="summary_large_image">
  
  <title>Huggingface transformers - Eric Chen&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"erichen0615.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":"G-YZ0RNYKG8D","gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jinghong Chen (Eric)</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Huggingface transformers">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-10-01 00:00" pubdate>
        October 1, 2021 am
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      13k words
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      16 minutes
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Huggingface transformers</h1>
            
            <div class="markdown-body">
              <p>Excerpts from <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/quicktour">https://huggingface.co/docs/transformers/quicktour</a></p>
<h1 id="Quick-Tour"><a href="#Quick-Tour" class="headerlink" title="Quick Tour"></a>Quick Tour</h1><p>Let’s have a quick look at the huggingface 🤗 Transformers library features. The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.</p>
<h2 id="Getting-started-with-a-pipeline"><a href="#Getting-started-with-a-pipeline" class="headerlink" title="Getting started with a pipeline"></a>Getting started with a pipeline</h2><p>Let’s see how this work for sentiment analysis (the other tasks are all covered in the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/task_summary.html">task summary</a>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&#x27;sentiment-analysis&#x27;</span>)<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&#x27;We are very happy to show you the 🤗 Transformers library.&#x27;</span>)<br>[&#123;<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>&#125;]<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>,<br><span class="hljs-meta">... </span>           <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">&#123;result[<span class="hljs-string">&#x27;label&#x27;</span>]&#125;</span>, with score: <span class="hljs-subst">&#123;<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)&#125;</span>&quot;</span>)<br>label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span><br>label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">transformers.pipeline(*task: <span class="hljs-built_in">str</span>*, *model: <span class="hljs-type">Optional</span> = <span class="hljs-literal">None</span>*, *config: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, transformers.configuration_utils.PretrainedConfig]] = <span class="hljs-literal">None</span>*, *tokenizer: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, transformers.tokenization_utils.PreTrainedTokenizer]] = <span class="hljs-literal">None</span>*, *feature_extractor: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, SequenceFeatureExtractor]] = <span class="hljs-literal">None</span>*, *framework: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>*, *revision: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>*, *use_fast: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>*, *use_auth_token: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>*, *model_kwargs: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>] = &#123;&#125;*, ***kwargs*) → transformers.pipelines.base.Pipeline<br></code></pre></td></tr></table></figure>

<p>[<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/_modules/transformers/pipelines.html#pipeline">SOURCE]</a></p>
<p>Utility factory method to build a <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.Pipeline"><code>Pipeline</code></a>.</p>
<p>Pipelines are made of:</p>
<blockquote>
<ul>
<li>A <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/tokenizer.html">tokenizer</a> in charge of mapping raw textual input to token.</li>
<li>A <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/model.html">model</a> to make predictions from the inputs.</li>
<li>Some (optional) post processing for enhancing model’s output.</li>
</ul>
</blockquote>
<p>See <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline">documentation</a></p>
<h2 id="Pretrained-Model"><a href="#Pretrained-Model" class="headerlink" title="Pretrained Model"></a>Pretrained Model</h2><p>The model and tokenizer are created using the <code>from_pretrained</code> method</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification<br><span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)<br></code></pre></td></tr></table></figure>

<h3 id="Using-the-tokenizer"><a href="#Using-the-tokenizer" class="headerlink" title="Using the tokenizer"></a>Using the tokenizer</h3><p>We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in words (or part of words, punctuation symbols, etc.) usually called tokens. There are multiple rules that can govern that process (you can learn more about them in the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/tokenizer_summary.html">tokenizer summary</a>), which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was pretrained.</p>
<p>The second step is to convert those tokens into numbers, to be able to build a tensor out of them and feed them to the model. To do this, the tokenizer has a <strong>vocab</strong>, which is the part we download when we instantiate it with the <code>from_pretrained</code> method, since we need to use the same vocab as when the model was pretrained.</p>
<p>To apply these steps on a given text, we can just feed it to our tokenizer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>This returns a dictionary string to list of ints. It contains the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/glossary.html#input-ids">ids of the tokens</a>, as mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/glossary.html#attention-mask">attention mask</a> that the model will use to have a better understanding of the sequence:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(inputs)<br>&#123;<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],<br> <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]&#125;<br></code></pre></td></tr></table></figure>

<p>You can pass a list of sentences directly to your tokenizer. If your goal is to send them through your model as a batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept and get tensors back. You can specify all of that to the tokenizer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(<br><span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],<br><span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,<br><span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,<br><span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,<br><span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span><br><span class="hljs-meta">... </span>)<br></code></pre></td></tr></table></figure>

<p>The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding token the model was pretrained with. The attention mask is also adapted to take the padding into account:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> pt_batch.items():<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;key&#125;</span>: <span class="hljs-subst">&#123;value.numpy().tolist()&#125;</span>&quot;</span>)<br>input_ids: [[<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">3246</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">2123</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1056</span>, <span class="hljs-number">5223</span>, <span class="hljs-number">2009</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]<br>attention_mask: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]<br></code></pre></td></tr></table></figure>

<h3 id="Using-the-model"><a href="#Using-the-model" class="headerlink" title="Using the model"></a>Using the model</h3><p>Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will contain all the relevant information the model needs. If you’re using a TensorFlow model, you can pass the dictionary keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding <code>**</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)<br></code></pre></td></tr></table></figure>

<p>Once your model is fine-tuned, you can save it with its tokenizer in the following way:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer.save_pretrained(save_directory)<br>model.save_pretrained(save_directory)<br></code></pre></td></tr></table></figure>

<p>You can then load this model back using the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel.from_pretrained"><code>from_pretrained()</code></a> method by passing the directory name instead of the model name. One cool feature of 🤗 Transformers is that you can easily switch between PyTorch and TensorFlow: <strong>any model saved as before can be loaded back either in PyTorch or TensorFlow</strong>. If you are loading a saved PyTorch model in a TensorFlow model, use <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/auto.html#transformers.TFAutoModel.from_pretrained"><code>from_pretrained()</code></a> like this:</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">from transformers import TFAutoModel<br>tokenizer = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">AutoTokenizer</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">save_directory</span>)</span><br>model = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">TFAutoModel</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">save_directory</span>, <span class="hljs-params">from_pt</span>=True)</span><br></code></pre></td></tr></table></figure>

<p>and if you are loading a saved TensorFlow model in a PyTorch model, you should use the following code:</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">from transformers import AutoModel<br>tokenizer = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">AutoTokenizer</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">save_directory</span>)</span><br>model = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">AutoModel</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-params">save_directory</span>, <span class="hljs-params">from_tf</span>=True)</span><br></code></pre></td></tr></table></figure>

<h3 id="Customizing-the-model"><a href="#Customizing-the-model" class="headerlink" title="Customizing the model"></a>Customizing the model</h3><p>If you want to change how the model itself is built, you can define a custom configuration class. Each architecture comes with its own relevant configuration. For example, <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/distilbert.html#transformers.DistilBertConfig"><code>DistilBertConfig</code></a> allows you to specify parameters such as the hidden dimension, dropout rate, etc for DistilBERT. If you do core modifications, like changing the hidden size, you won’t be able to use a pretrained model anymore and will need to train from scratch. You would then instantiate the model directly from this configuration.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification<br><span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig(n_heads=<span class="hljs-number">8</span>, dim=<span class="hljs-number">512</span>, hidden_dim=<span class="hljs-number">4</span>*<span class="hljs-number">512</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification(config)<br></code></pre></td></tr></table></figure>

<p>For something that only changes the head of the model (for instance, the number of labels), <strong>you can still use a pretrained model for the body.</strong> For instance, let’s define a classifier for 10 different labels using a pretrained body. Instead of creating a new configuration with all the default values just to change the number of labels, we can instead pass any argument a configuration would take to the <code>from_pretrained()</code> method and it will update the default configuration appropriately:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification<br><span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=<span class="hljs-number">10</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(model_name)<br></code></pre></td></tr></table></figure>

<h1 id="Fine-tuning-a-pretrained-model"><a href="#Fine-tuning-a-pretrained-model" class="headerlink" title="Fine-tuning a pretrained model"></a>Fine-tuning a pretrained model</h1><p>In PyTorch, there is no generic training loop so the 🤗 Transformers library provides an API with the class <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer</code></a> to let you fine-tune or train a model from scratch easily. Then we will show you how to alternatively write the whole training loop in PyTorch.</p>
<h2 id="Preparing-the-dataset"><a href="#Preparing-the-dataset" class="headerlink" title="Preparing the dataset"></a>Preparing the dataset</h2><p>We will use the <a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/">🤗 Datasets</a> library to download and preprocess the IMDB datasets. We will go over this part pretty quickly. Since the focus of this tutorial is on training, you should refer to the 🤗 Datasets <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/">documentation</a> or the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/preprocessing.html">Preprocessing data</a> tutorial for more information.</p>
<p>First, we can use the <code>load_dataset</code> function to download and cache the dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><br>raw_datasets = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>To preprocess our data, we will need a tokenizer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)<br>inputs = tokenizer(sentences, padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_function</span>(<span class="hljs-params">examples</span>):</span><br>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># transform the dataset</span><br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br><br></code></pre></td></tr></table></figure>

<h2 id="Fine-tuning-with-Trainer-API"><a href="#Fine-tuning-with-Trainer-API" class="headerlink" title="Fine-tuning with Trainer API"></a>Fine-tuning with Trainer API</h2><p>First, let’s define our model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>

<p>Then, to define our <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer</code></a>, we will need to instantiate a <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a>. This class contains all the hyperparameters we can tune for the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer</code></a> or the flags to activate the different training options it supports. Let’s begin by using all the defaults, the only thing we then have to provide is a directory in which the checkpoints will be saved:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments<br><br>training_args = TrainingArguments(<span class="hljs-string">&quot;test_trainer&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>Then we can instantiate a <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer</code></a> like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer<br><br>trainer = Trainer(<br>    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset<br>)<br></code></pre></td></tr></table></figure>

<p>To fine-tune our model, we just need to call</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer.train()<br></code></pre></td></tr></table></figure>

<p>To have the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer</code></a> compute and report metrics, we need to give it a <code>compute_metrics</code> function that takes predictions and labels (grouped in a namedtuple called <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/internal/trainer_utils.html#transformers.EvalPrediction"><code>EvalPrediction</code></a>) and return a dictionary with string items (the metric names) and float values (the metric values).</p>
<p>The 🤗 Datasets library provides an easy way to get the common metrics used in NLP with the <code>load_metric</code> function. here we simply use accuracy. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric<br><br>metric = load_metric(<span class="hljs-string">&quot;accuracy&quot;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):</span><br>    logits, labels = eval_pred<br>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)<br></code></pre></td></tr></table></figure>

<p>To check if this works on practice, let’s create a new <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer</code></a> with our fine-tuned model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    train_dataset=small_train_dataset,<br>    eval_dataset=small_eval_dataset,<br>    compute_metrics=compute_metrics,<br>)<br>trainer.evaluate()<br></code></pre></td></tr></table></figure>

<p>If you want to fine-tune your model and regularly report the evaluation metrics (for instance at the end of each epoch), here is how you should define your training arguments:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments<br><br>training_args = TrainingArguments(<span class="hljs-string">&quot;test_trainer&quot;</span>, evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>See the documentation of <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a> for more options.</p>
<h2 id="Fine-tuning-with-native-PyTorch"><a href="#Fine-tuning-with-native-PyTorch" class="headerlink" title="Fine-tuning with native PyTorch"></a>Fine-tuning with native PyTorch</h2><p>We just need to apply a bit of post-processing to our <code>tokenized_datasets</code> before doing that to:</p>
<ul>
<li>remove the columns corresponding to values the model does not expect (here the <code>&quot;text&quot;</code> column)</li>
<li>rename the column <code>&quot;label&quot;</code> to <code>&quot;labels&quot;</code> (because the model expect the argument to be named <code>labels</code>)</li>
<li>set the format of the datasets so they return PyTorch Tensors instead of lists.</li>
</ul>
<p>Our <code>tokenized_datasets</code> has one method for each of those steps:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&quot;text&quot;</span>])<br>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)<br>tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)<br><br>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br></code></pre></td></tr></table></figure>

<p>We can easily define our dataloaders:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">8</span>)<br>eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="hljs-number">8</span>)<br></code></pre></td></tr></table></figure>

<p>Next, we define our model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>

<p>We are almost ready to write our training loop. The only two things to add are an <strong>optimizer</strong> and a <strong>learning rate scheduler</strong>. The default optimizer used by the <code>Trainer</code> is <code>AdamW</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler<br><br>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)<br>num_epochs = <span class="hljs-number">3</span><br>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)<br>lr_scheduler = get_scheduler(<br>    <span class="hljs-string">&quot;linear&quot;</span>,<br>    optimizer=optimizer,<br>    num_warmup_steps=<span class="hljs-number">0</span>,<br>    num_training_steps=num_training_steps<br>)<br></code></pre></td></tr></table></figure>

<p>We will want to use the GPU if we have access to one. To do this, we define a <code>device</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>model.to(device)<br></code></pre></td></tr></table></figure>

<p>We now are ready to train! To get some sense of when it will be finished, we add a progress bar over our number of training steps, using the tqdm library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))<br><br>model.train()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:<br>        batch = &#123;k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()&#125;<br>        outputs = model(**batch)<br>        loss = outputs.loss<br>        loss.backward()<br><br>        optimizer.step()<br>        lr_scheduler.step()<br>        optimizer.zero_grad()<br>        progress_bar.update(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<p>Now to check the results, we need to write the <strong>evaluation loop</strong>. Like in the <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/training.html#trainer">trainer section</a> we will use a metric from the datasets library. Here we accumulate the predictions at each batch before computing the final result when the loop is finished.</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">metric= load<span class="hljs-constructor">_metric(<span class="hljs-string">&quot;accuracy&quot;</span>)</span><br>model.eval<span class="hljs-literal">()</span><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:<br>    batch = &#123;k: v.<span class="hljs-keyword">to</span>(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items<span class="hljs-literal">()</span>&#125;<br>    <span class="hljs-keyword">with</span> torch.no<span class="hljs-constructor">_grad()</span>:<br>        outputs = model(**batch)<br><br>    logits = outputs.logits<br>    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)<br>    metric.add<span class="hljs-constructor">_batch(<span class="hljs-params">predictions</span>=<span class="hljs-params">predictions</span>, <span class="hljs-params">references</span>=<span class="hljs-params">batch</span>[<span class="hljs-string">&quot;labels&quot;</span>])</span><br><br>metric.compute<span class="hljs-literal">()</span><br></code></pre></td></tr></table></figure>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    Jinghong Chen @2021
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/10/01/Huggingface-datasets/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Huggingface datasets</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/07/01/Big-Data-Analysis-with-Scala-and-Spark/">
                        <span class="hidden-mobile">Big Data Analysis with Scala and Spark</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            
            <span id="busuanzi_value_site_pv"></span>
             visits
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            
            <span id="busuanzi_value_site_uv"></span>
             visitors
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  








  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'G-YZ0RNYKG8D', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
