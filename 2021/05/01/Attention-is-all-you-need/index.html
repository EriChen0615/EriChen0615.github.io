

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="The personal blog of Jinghong Chen (Eric). On Machine Learning, Programming and beyond">
  <meta name="author" content="Jinghong Chen">
  <meta name="keywords" content="Machine Learning Jinghong Chen Eric Programming blog">
  <meta name="description" content="Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations $(x_1; …; x_n$) to a sequen">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention is all you need">
<meta property="og:url" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/index.html">
<meta property="og:site_name" content="Eric Chen&#39;s Blog">
<meta property="og:description" content="Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations $(x_1; …; x_n$) to a sequen">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512134051150.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512135156753.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512141759031.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512145743785.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512151235365.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512151106332.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210513004707847.png">
<meta property="og:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512152746147.png">
<meta property="article:published_time" content="2021-05-01T00:00:00.000Z">
<meta property="article:modified_time" content="2022-03-05T00:55:30.878Z">
<meta property="article:author" content="Jinghong Chen">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://erichen0615.github.io/2021/05/01/Attention-is-all-you-need/image-20210512134051150.png">
  
  <title>Attention is all you need - Eric Chen&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"erichen0615.github.io","root":"/","version":"1.8.12","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":"G-YZ0RNYKG8D","gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jinghong Chen (Eric)</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Attention is all you need">
              
                Attention is all you need
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-05-01 00:00" pubdate>
        May 1, 2021 am
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.5k words
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      6 minutes
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Attention is all you need</h1>
            
            <div class="markdown-body">
              <h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations $(x_1; …; x_n$) to a sequence of continuous representations $z = (z_1; …; z_n)$. Given $z$, the decoder then generates an output sequence $(y_1; …; y_m)$ of symbols one element at a time.</p>
<p>The <em>Transformer</em> follows this overall architecture using <strong>stacked self-attention</strong> and <strong>point-wise, fully connected layers</strong> for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p>
<p><img src="image-20210512134051150.png" srcset="/img/loading.gif" lazyload alt="image-20210512134051150"></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><strong>An attention can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values and outputs are all vectors.</strong> The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<p><img src="image-20210512135156753.png" srcset="/img/loading.gif" lazyload alt="image-20210512135156753"></p>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the</p>
<p><em>query</em> with all <em>keys</em>, divide each by $\sqrt{d_k}$, and <strong>apply a softmax function</strong> to obtain the weights on the values.</p>
<p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$ . We compute the matrix of outputs as: $\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</p>
<p><img src="image-20210512141759031.png" srcset="/img/loading.gif" lazyload alt="image-20210512141759031"></p>
<blockquote>
<p>Note that the matrices are <em>row-based</em>, that is, the row vectors are concatenated to form a matrix.</p>
</blockquote>
<p>We normalize the weights before passing it through a softmax because as the dimension of queries and keys $d_v$ grows, their dot product tends to get large in magnitude, pushing the softmax into saturation (i.e., small gradient).</p>
<h3 id="Intepretation-as-Logistic-Regression"><a href="#Intepretation-as-Logistic-Regression" class="headerlink" title="Intepretation as Logistic Regression"></a>Intepretation as Logistic Regression</h3><p>The corresponding output $v$ to specific output $q$ is given by $\sum_j \lambda_jv_j$, where $\sum_j \lambda_j = 0$. We can let $\lambda_j = p(v_j|q)$, and the attention mechanism is essential a <strong>multi-class logistic regression</strong> problem, where the input is the query $q$ and the output is the probability distribution of likelihood of the values. This can be observed by inspecting: $p(v_j|q) = \frac{-\exp(-q\cdot k_j)}{\sum_k\exp(-q\cdot k_k)}$ takes the form of logistic regression.</p>
<p>Hence the keys $k_j$ can be viewed as the parameters to be learnt from the logistic model. It is equivalent to a no-hidden layer neural network, where the input layer has <strong>linear activation</strong> and the output layer is a softmax layer.</p>
<img src="image-20210512145743785.png" srcset="/img/loading.gif" lazyload alt="image-20210512145743785" style="zoom:50%;" />

<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to <strong>linearly project</strong> the queries, keys and values $h$ times with different, learned linear projections to $d_k, d_k$ and $d_v$ dimensions, respectively.</p>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, which is not achievable with single attention head. The outputs from each head are concatanated to form the final output.</p>
<p>Mathematically: </p>
<p><img src="image-20210512151235365.png" srcset="/img/loading.gif" lazyload alt="image-20210512151235365"></p>
<blockquote>
<p>This essentially insert a hidden layer with linear activation. The weights of each head are separately learnt.</p>
</blockquote>
<p><img src="image-20210512151106332.png" srcset="/img/loading.gif" lazyload alt="image-20210512151106332"></p>
<h3 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h3><p>The docoder should be prevented from seeing downstream (future) words that have not been generated during training. To do this, we can adopt masking, which can be done by adding a look-ahead mask as follows:</p>
<p><img src="image-20210513004707847.png" srcset="/img/loading.gif" lazyload alt="image-20210513004707847"></p>
<p>The masked scores is then fed into a softmax layer, zeroing out all the $-\infty$ values.</p>
<h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between (i.e., <strong>one hidden layer</strong>).</p>
<h2 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h2><p><code>Using the output embedding to improve language models</code></p>
<h2 id="Positional-Encoding-（-）"><a href="#Positional-Encoding-（-）" class="headerlink" title="Positional Encoding （*）"></a>Positional Encoding （*）</h2><p><strong>Since the model contains no recurrence and no convolution</strong>, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. In this work, we use sine and cosine functions of different frequencies:</p>
<p><img src="image-20210512152746147.png" srcset="/img/loading.gif" lazyload alt="image-20210512152746147"></p>
<h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>We employ three types of regularization during training:</p>
<ol>
<li><p><strong>Residual Dropout</strong>: We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, <em>we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.</em> For the base model, we use a rate of $P_{drop} = 0.1$.</p>
</li>
<li><p><strong>Label Smoothing</strong>: During training, we employed label smoothing of value $ls = 0.1$. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p>
</li>
</ol>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h1 id="The-Animated-Transformer"><a href="#The-Animated-Transformer" class="headerlink" title="The Animated Transformer"></a>The Animated Transformer</h1><p>See this <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4Bdc55j80l8">video</a> for animation. The first 30 seconds of the video illustrates how a transformer works and what it means to <em>attend to</em> different words. Fast forward to [10:00] to see how the decoder does. [11:00] explains how masking works.</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Machine-Learning/">Machine Learning</a>
                    
                      <a class="hover-with-bg" href="/tags/NLP/">NLP</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    Jinghong Chen @2021-2022
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/06/01/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">计算广告</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/12/01/A-tour-of-Cpp/">
                        <span class="hidden-mobile">A Tour of C++</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            
            <span id="busuanzi_value_site_pv"></span>
             visits
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            
            <span id="busuanzi_value_site_uv"></span>
             visitors
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>








  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  








  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'G-YZ0RNYKG8D', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
